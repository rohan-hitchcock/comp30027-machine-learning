{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2020 Semester 1\n",
    "\n",
    "## Assignment 1: Naive Bayes Classifiers\n",
    "\n",
    "###### Submission deadline: 7 pm, Monday 20 Apr 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict as dd\n",
    "from math import log\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "#TODO: REMOVE THIS \n",
    "import os\n",
    "os.chdir(\"/home/rohan/Repositories/comp30027-machine-learning/ass1/src/submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RANDOM_SEED = 2\n",
    "NUM_PARTITIONS = 10\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'instances': 8124, 'columns': 23, 'class_col': 0, 'missing_values': ['?'], 'discrete': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], 'numeric': []}\nData summary: -------------------------------------------------------\n0: (class) dtype=object\nvalues (2): ['p' 'e']\nmissing: 0 / 8124\n\n1: (discrete) dtype=object\nvalues (6): ['x' 'b' 's' 'f' 'k' 'c']\nmissing: 0 / 8124\n\n2: (discrete) dtype=object\nvalues (4): ['s' 'y' 'f' 'g']\nmissing: 0 / 8124\n\n3: (discrete) dtype=object\nvalues (10): ['n' 'y' 'w' 'g' 'e' 'p' 'b' 'u' 'c' 'r']\nmissing: 0 / 8124\n\n4: (discrete) dtype=object\nvalues (2): ['t' 'f']\nmissing: 0 / 8124\n\n5: (discrete) dtype=object\nvalues (9): ['p' 'a' 'l' 'n' 'f' 'c' 'y' 's' 'm']\nmissing: 0 / 8124\n\n6: (discrete) dtype=object\nvalues (2): ['f' 'a']\nmissing: 0 / 8124\n\n7: (discrete) dtype=object\nvalues (2): ['c' 'w']\nmissing: 0 / 8124\n\n8: (discrete) dtype=object\nvalues (2): ['n' 'b']\nmissing: 0 / 8124\n\n9: (discrete) dtype=object\nvalues (12): ['k' 'n' 'g' 'p' 'w' 'h' 'u' 'e' 'b' 'r' 'y' 'o']\nmissing: 0 / 8124\n\n10: (discrete) dtype=object\nvalues (2): ['e' 't']\nmissing: 0 / 8124\n\n11: (discrete) dtype=object\nvalues (5): ['e' 'c' 'b' 'r' nan]\nmissing: 2480 / 8124\n\n12: (discrete) dtype=object\nvalues (4): ['s' 'f' 'k' 'y']\nmissing: 0 / 8124\n\n13: (discrete) dtype=object\nvalues (4): ['s' 'f' 'y' 'k']\nmissing: 0 / 8124\n\n14: (discrete) dtype=object\nvalues (9): ['w' 'g' 'p' 'n' 'b' 'e' 'o' 'c' 'y']\nmissing: 0 / 8124\n\n15: (discrete) dtype=object\nvalues (9): ['w' 'p' 'g' 'b' 'n' 'e' 'y' 'o' 'c']\nmissing: 0 / 8124\n\n16: (discrete) dtype=object\nvalues (1): ['p']\nmissing: 0 / 8124\n\n17: (discrete) dtype=object\nvalues (4): ['w' 'n' 'o' 'y']\nmissing: 0 / 8124\n\n18: (discrete) dtype=object\nvalues (3): ['o' 't' 'n']\nmissing: 0 / 8124\n\n19: (discrete) dtype=object\nvalues (5): ['p' 'e' 'l' 'f' 'n']\nmissing: 0 / 8124\n\n20: (discrete) dtype=object\nvalues (9): ['k' 'n' 'u' 'h' 'w' 'r' 'o' 'y' 'b']\nmissing: 0 / 8124\n\n21: (discrete) dtype=object\nvalues (6): ['s' 'n' 'a' 'v' 'y' 'c']\nmissing: 0 / 8124\n\n22: (discrete) dtype=object\nvalues (7): ['u' 'g' 'm' 'd' 'p' 'w' 'l']\nmissing: 0 / 8124\n\n---------------------------------------------------------------------\n     0  1  2  3  4  5  6  7  8  9   ... 13 14 15 16 17 18 19 20 21 22\n0     p  x  s  n  t  p  f  c  n  k  ...  s  w  w  p  w  o  p  k  s  u\n1     e  x  s  y  t  a  f  c  b  k  ...  s  w  w  p  w  o  p  n  n  g\n2     e  b  s  w  t  l  f  c  b  n  ...  s  w  w  p  w  o  p  n  n  m\n3     p  x  y  w  t  p  f  c  n  n  ...  s  w  w  p  w  o  p  k  s  u\n4     e  x  s  g  f  n  f  w  b  k  ...  s  w  w  p  w  o  e  n  a  g\n...  .. .. .. .. .. .. .. .. .. ..  ... .. .. .. .. .. .. .. .. .. ..\n8119  e  k  s  n  f  n  a  c  b  y  ...  s  o  o  p  o  o  p  b  c  l\n8120  e  x  s  n  f  n  a  c  b  y  ...  s  o  o  p  n  o  p  b  v  l\n8121  e  f  s  n  f  n  a  c  b  n  ...  s  o  o  p  o  o  p  b  c  l\n8122  p  k  y  n  f  y  f  c  n  b  ...  k  w  w  p  w  o  e  w  v  l\n8123  e  x  s  n  f  n  a  c  b  y  ...  s  o  o  p  o  o  p  o  c  l\n\n[8124 rows x 23 columns]\n"
    }
   ],
   "source": [
    "def preprocess(filepath, header=None, config_file=\"config.json\"):\n",
    "\n",
    "    #load configuration settings from file\n",
    "    with open(config_file) as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    #extract the name of the dataset from the filepath\n",
    "    pth, fname = os.path.split(filepath)    #pylint:disable=unused-variable\n",
    "    fname, ext = os.path.splitext(fname)    #pylint:disable=unused-variable\n",
    "\n",
    "    if fname not in config:\n",
    "        raise NotImplementedError(f\"The \\'{fname}\\' dataset is not supported.\")\n",
    "    \n",
    "    data_config = config[fname]\n",
    "\n",
    "    print(data_config)\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        filepath, \n",
    "        header=header, \n",
    "        names=range(data_config['columns']),\n",
    "        na_values=data_config['missing_values']\n",
    "    )\n",
    "    \n",
    "    df = df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    \n",
    "    return df, data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Convenient way to store the data for a learned Naive Bayes model. \n",
    "\n",
    "    Fields:\n",
    "        discrete:     discrete probability data. A triple-nested dictionary\n",
    "                      such that discrete[da][c][x] = P(x|c) where x is a \n",
    "                      value of a discrete attribute da.\n",
    "        numeric:      numeric probability data. A dictionary (keyed by attribute \n",
    "                      name) of tuples (mu, sigma) where each is an array of the \n",
    "                      same length as class_vals storing the mean and standard \n",
    "                      deviation of this attribute for each class value.\n",
    "        class_vals:   an array of possible values of the class to predict\n",
    "        class_priors: an array the same length of class_vals such that the ith\n",
    "                      element is the probability of class_vals[i]\n",
    "\"\"\"\n",
    "NBModel = namedtuple(\"NBModel\", ['discrete', 'numeric',\n",
    "                                 'class_vals', 'class_priors'])\n",
    "\n",
    "def discrete_probabilities(obs, vals):\n",
    "    \"\"\" Estimates the probability of observing each value of a discrete \n",
    "        phenomena, based on a series of observations.\n",
    "\n",
    "        Args:\n",
    "            obs: a numpy array of observations where each element is contained \n",
    "            in vals\n",
    "            vals: an iterable of possibly observable values\n",
    "\n",
    "        Returns:\n",
    "            A numpy array a where a[i] is the probability of observing vals[i]\n",
    "    \"\"\"\n",
    "    return np.array([np.count_nonzero((obs == v)) / len(obs) for v in vals])\n",
    "\n",
    "\n",
    "def laplace_smoothing(n_attr_obs, n_class_obs, n_attr_vals, alpha):\n",
    "    \"\"\" When missing values arise, increases all counts by alpha (eg, missing values\n",
    "        are now seen once. 1->2, 2->3 etc.\n",
    "\n",
    "        Args:\n",
    "            n_attr_obs: number of attribute observations\n",
    "            n_class_obs: number of class observations\n",
    "            n_attr_vals: number of attribute values\n",
    "            alpha: laplace smoothing constant\n",
    "            \n",
    "\n",
    "        Returns:\n",
    "            Smoothed probability for that attribute and class\n",
    "    \"\"\"\n",
    "    return (n_attr_obs + alpha) / (n_class_obs + alpha * n_attr_vals)\n",
    "\n",
    "def train_discrete_standard(df, class_attr, class_vals, d_attr, eps=0):\n",
    "    \"\"\" For training Naive Bayes on a discrete attribute, with no / simple \n",
    "        smoothing.\n",
    "\n",
    "        For each class value computes the conditional probability of observing \n",
    "        each attribute value given that class value. \n",
    "\n",
    "        Args:\n",
    "            df: a pd.DataFrame\n",
    "            class_attr: the column in df of the class to predict\n",
    "            class_vals: the possible values of class_attr\n",
    "            d_attr: the column in df of the discrete attribute\n",
    "            eps: (optional) a value to replace any zero probabilities with.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary, keyed by class values, of dictonaries storing the \n",
    "            conditional probability of observing each attribute value\n",
    "    \"\"\"\n",
    "\n",
    "    attr_vals = df[d_attr].unique()\n",
    "\n",
    "    params = dict()\n",
    "    for cv in class_vals:\n",
    "\n",
    "        params[cv] = dict()\n",
    "        cv_obs = df[df[class_attr] == cv]\n",
    "        for av in attr_vals:\n",
    "            num_av_obs = np.count_nonzero((cv_obs[d_attr] == av).to_numpy())\n",
    "\n",
    "            pval = num_av_obs / cv_obs.shape[0]\n",
    "\n",
    "            params[cv][av] = pval if pval != 0 else eps\n",
    "\n",
    "    return params\n",
    "\n",
    "def train_discrete_laplace(df, class_attr, class_vals, d_attr, alpha=1):\n",
    "    \"\"\" For training Naive Bayes on a discrete attribute, with laplace smoothing\n",
    "\n",
    "        For each class value computes the conditional probability of observing \n",
    "        each attribute value given that class value. \n",
    "\n",
    "        Args:\n",
    "            df: a pd.DataFrame\n",
    "            class_attr: the column in df of the class to predict\n",
    "            class_vals: the possible values of class_attr\n",
    "            d_attr: the column in df of the discrete attribute\n",
    "            alpha: the alpha value in laplace smoothing\n",
    "\n",
    "        Returns:\n",
    "            A dictionary, keyed by class values, of dictonaries storing the \n",
    "            conditional probability of observing each attribute value\n",
    "    \"\"\"\n",
    "\n",
    "    attr_vals = df[d_attr].unique()\n",
    "\n",
    "    params = dict()\n",
    "    for cv in class_vals:\n",
    "\n",
    "        params[cv] = dict()\n",
    "        cv_obs = df[df[class_attr] == cv]\n",
    "        for av in attr_vals:\n",
    "            num_av_obs = np.count_nonzero((cv_obs[d_attr] == av).to_numpy())\n",
    "            params[cv][av] = laplace_smoothing(num_av_obs, cv_obs.shape[0], len(attr_vals), alpha)\n",
    "\n",
    "    return params\n",
    "\n",
    "def train_gaussian(df, class_attr, class_values, num_attr):\n",
    "    \"\"\" For training Naive Bayes on a numeric attribute.\n",
    "    \n",
    "        Calculates the mean and standard deviation of a numeric attribute for \n",
    "        each class. \n",
    "\n",
    "        Args:\n",
    "            df: A pd.DataFrame \n",
    "            class_attr: the column in df of the class we are predicting\n",
    "            class_values: an iterable of each possible value of class_attr\n",
    "            num_attr: a numeric attribute in df\n",
    "\n",
    "        Returns:\n",
    "            A dictionary (keyed by the class values) of tuples (mean, std)\n",
    "    \"\"\"\n",
    "\n",
    "    means = np.empty(len(class_values))\n",
    "    stdevs = np.empty(len(class_values))\n",
    "    for i, cv in enumerate(class_values):\n",
    "        # A Series which is True wherever cv was observed and false otherwise\n",
    "        cv_obs = df[class_attr] == cv\n",
    "\n",
    "        # TODO: write our own functions?\n",
    "        # by default Series.mean and Series.std will skip missing vals\n",
    "        means[i] = df[num_attr][cv_obs].mean()\n",
    "        stdevs[i] = df[num_attr][cv_obs].std()\n",
    "\n",
    "    return means, stdevs\n",
    "\n",
    "def train(df, discrete_attrs, numeric_attrs, class_name,\n",
    "          train_discrete=train_discrete_standard, train_numeric=train_gaussian):\n",
    "    \"\"\" Produce a Naive Bayes model for a given dataset.\n",
    "    \n",
    "        Args:\n",
    "            df: a pd.DataFrame of training data\n",
    "            discrete_attrs: a list of discrete attribute names of df\n",
    "            numeric_attrs: a list of numeric attribute names of df\n",
    "            class_name: the name in df of the class to predict\n",
    "            train_discrete: a callable \n",
    "                train_discrete(df, class_name, class_vals, attr) which calculates\n",
    "                the conditional probabilities of a discrete attribute attr\n",
    "            train_numeric: a callable \n",
    "                train_numeric(df, class_name, class_vals, attr) which calculates\n",
    "                the means and standard deviations of a numeric attribute attr\n",
    "        \n",
    "        Returns:\n",
    "            A NBModel object\n",
    "    \"\"\"\n",
    "    class_vals = df[class_name].unique()\n",
    "\n",
    "    discrete = dict()\n",
    "    for da in discrete_attrs:\n",
    "        discrete[da] = train_discrete(df, class_name, class_vals, da)\n",
    "\n",
    "    numeric = dict()\n",
    "    for na in numeric_attrs:\n",
    "        numeric[na] = train_numeric(df, class_name, class_vals, na)\n",
    "\n",
    "    class_priors = discrete_probabilities(df[class_name].to_numpy(), class_vals)\n",
    "\n",
    "    return NBModel(discrete, numeric, class_vals, class_priors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def guassian_pdf(x, mu, sigma):\n",
    "    \"\"\"Gaussian probability density function. Assumes a gaussian distribution for numeric attributes and calculates \n",
    "        the probabilities based on the density function\n",
    "        Args:\n",
    "            x: attribute value\n",
    "            mu: Attribute mean\n",
    "            sigma: Attribute Standard Deviation\n",
    "        Returns:\n",
    "            Likely hood of observing x\n",
    "    \"\"\"\n",
    "    return (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * (((x - mu) / sigma) ** 2))\n",
    "\n",
    "def loglim(x):\n",
    "    \"\"\" Returns log(x) for positive x and -float(\"inf\") otherwise\"\"\"\n",
    "    return log(x) if x > 0 else -float(\"inf\")\n",
    "\n",
    "def predict(df, nbm):\n",
    "    \"\"\" Predict class labels using a Naive Bayes model.\n",
    "\n",
    "        Args:\n",
    "            df: a pd.DataFrame storing training instances.\n",
    "            nbm: a NBModel object trained to predict on instances in df\n",
    "        Returns:\n",
    "            A pd.Series of class labels, with index equal to df.index\n",
    "    \"\"\"\n",
    "\n",
    "    # numeric and discrete attributes\n",
    "    n_attrs = list(nbm.numeric.keys())\n",
    "    d_attrs = list(nbm.discrete.keys())\n",
    "\n",
    "    predictions = pd.Series(\n",
    "        np.empty(len(df), \n",
    "        dtype=nbm.class_vals.dtype), \n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "        class_likelyhoods = np.empty(len(nbm.class_vals))\n",
    "        for i, cv in enumerate(nbm.class_vals):\n",
    "\n",
    "            cl = loglim(nbm.class_priors[i])\n",
    "\n",
    "            if d_attrs:\n",
    "                for a, x in zip(d_attrs, row[d_attrs]):\n",
    "\n",
    "                    # TODO: this is not correct, need to change training?\n",
    "                    if pd.notna(x) and x in nbm.discrete[a][cv]:\n",
    "                        cl += loglim(nbm.discrete[a][cv][x])\n",
    "\n",
    "            if n_attrs:\n",
    "                for a, x in zip(n_attrs, row[n_attrs]):\n",
    "                    if pd.notna(x):\n",
    "                        # means and standard deviations for this numeric attribute\n",
    "                        means, stdevs = nbm.numeric[a]\n",
    "                        cl += loglim(guassian_pdf(x, means[i], stdevs[i]))\n",
    "\n",
    "            class_likelyhoods[i] = cl\n",
    "\n",
    "        predictions[idx] = nbm.class_vals[np.argmax(class_likelyhoods)]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should evaluate the prediction performance by comparing your model’s class outputs to ground\n",
    "# truth labels\n",
    "\n",
    "def evaluate(truth_labels, predictions, f_score_beta=1):\n",
    "    \"\"\" Evaluates a prediction compared to the ground truth labels according to \n",
    "        a number of different metrics.\"\"\"\n",
    "    assert (len(truth_labels) == len(predictions))\n",
    "    a = accuracy(truth_labels, predictions)\n",
    "    cm = confusion_matrix(truth_labels, predictions)\n",
    "    p = precision(cm)\n",
    "    r = recall(cm)\n",
    "    f = f_score(p, r, f_score_beta)\n",
    "    return a, p, r, f\n",
    "\n",
    "def accuracy(class_col, ybar):\n",
    "    results = np.array(class_col) == ybar\n",
    "    return np.count_nonzero(results == True) / len(ybar)\n",
    "\n",
    "def precision(cm):\n",
    "    \"\"\"Precision of each class, returned as an average weighted by the number of\n",
    "    instances in each class\"\"\"\n",
    "    fp = np.sum(cm, axis=0)\n",
    "    precisions = np.diag(cm) / np.where(fp == 0, 1, fp)\n",
    "    weights = np.sum(cm, axis=1) / cm.sum()\n",
    "    return np.sum(precisions * weights)\n",
    "\n",
    "def recall(cm):\n",
    "    \"\"\"Recall of each class, returned as an average weighted by the number of\n",
    "    instances in each class\"\"\"\n",
    "    fp = np.sum(cm, axis=1)\n",
    "    recalls = np.diag(cm) / np.where(fp == 0, 1, fp)\n",
    "    weights = np.sum(cm, axis=1) / cm.sum()\n",
    "    return np.sum(recalls * weights)\n",
    "\n",
    "def f_score(p, r, beta):\n",
    "    return ((1 + beta * beta) * p * r) / ((beta * beta * p) + r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "\n",
    "\n",
    "If you are in a group of 1, you will respond to question (1), and **one** other of your choosing (two responses in total).\n",
    "\n",
    "If you are in a group of 2, you will respond to question (1) and question (2), and **two** others of your choosing (four responses in total). \n",
    "\n",
    "A response to a question should take about 100–250 words, and make reference to the data wherever possible.\n",
    "\n",
    "#### NOTE: you may develope codes or functions in respond to the question, but your formal answer should be added to a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Try discretising the numeric attributes in these datasets and treating them as discrete variables in the na¨ıve Bayes classifier. You can use a discretisation method of your choice and group the numeric values into any number of levels (but around 3 to 5 levels would probably be a good starting point). Does discretizing the variables improve classification performance, compared to the Gaussian na¨ıve Bayes approach? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer as KBD\n",
    "def discretise_equal_width(numeric_series, nbins):\n",
    "    \"\"\" Discretises a numeric data series accoding to equal-width discretisation\n",
    "        \n",
    "        Assumes no missing values.\n",
    "\n",
    "        Args:\n",
    "            numeric_series: a pd.Series containing numeric data\n",
    "            nbins: the number of discrete classes for the data\n",
    "            \n",
    "        Returns:\n",
    "            A pd.Series with nbins unique values.\n",
    "    \"\"\"\n",
    "\n",
    "    est = KBD(n_bins=nbins, encode='ordinal', strategy='uniform')\n",
    "    est.fit(numeric_series)\n",
    "    return pd.DataFrame(est.transform(numeric_series), dtype='int64', columns=numeric_series.columns)\n",
    "\n",
    "\n",
    "def discretise_k_means(numeric_series, k):\n",
    "    \"\"\" Discretises a numeric data series according to k-means \n",
    "\n",
    "        Assumes no missing values.\n",
    "\n",
    "        Args:\n",
    "            numeric_series: a pd.Series containing numeric data\n",
    "            k: the number of discrete categories\n",
    "            repeates: the number of repeats of k-means\n",
    "        \n",
    "        Returns:\n",
    "            A pd.Series with k unique values of the same index as numeric_series\n",
    "    \"\"\"\n",
    "\n",
    "    est = KBD(n_bins=k, encode='ordinal', strategy='kmeans')\n",
    "    est.fit(numeric_series)\n",
    "    return pd.DataFrame(est.transform(numeric_series), dtype='int64', columns=numeric_series.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "Implement a baseline model (e.g., random or 0R) and compare the performance of the na¨ıve Bayes classifier to this baseline on multiple datasets. Discuss why the baseline performance varies across datasets, and to what extent the na¨ıve Bayes classifier improves on the baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_zero_r(training_class_obs, num_to_classify):\n",
    "    \"\"\" Classifies according to the most common class in the training data.\n",
    "\n",
    "        Args:\n",
    "            training_class_obs: the observations of class in the training data\n",
    "            num_to_classify: the number of instances to classify\n",
    "\n",
    "        Returns:\n",
    "            A numpy array of length num_to_classify containing the most frequent\n",
    "            value in training_class_obs\n",
    "    \"\"\"\n",
    "    values, counts = np.unique(training_class_obs, return_counts=True)\n",
    "    ind = np.argmax(counts)\n",
    "    return np.full(num_to_classify, values[ind])\n",
    "\n",
    "def classify_random(training_class_obs, num_to_classify):\n",
    "    \"\"\" Classifies data at random according to the relative frequencies of \n",
    "        each class in the training observations.\n",
    "\n",
    "        Args:\n",
    "            training_class_obs: the observations of class in the training data\n",
    "            num_to_classify: the number of instances to classify\n",
    "\n",
    "        Returns:\n",
    "            An array of length num_to_clasify of class values chosen at the \n",
    "            same probability as in training_class_obs\n",
    "    \"\"\"\n",
    "    class_vals = np.unique(training_class_obs)\n",
    "    class_probs = nb.discrete_probabilities(training_class_obs, class_vals)\n",
    "    return np.random.choice(class_vals, size=num_to_classify, p=class_probs)\n",
    "\n",
    "def classify_uniform(training_class_obs, num_to_classify):\n",
    "    \"\"\" Classifies data uniformly at random.\n",
    "\n",
    "        Args:\n",
    "            training_class_obs: the observations of class in the training data\n",
    "            num_to_classify: the number of instances to classify\n",
    "\n",
    "        Returns:\n",
    "            An array of length num_to_clasify of class values where each value\n",
    "            was chosen with uniform probability.\n",
    "    \"\"\"\n",
    "    class_vals = np.unique(training_class_obs)\n",
    "    return np.random.choice(class_vals, size=num_to_classify)\n",
    "\n",
    "def classify_one_r(training_df, class_name, testing_df):\n",
    "    \"\"\" Classifies testing_df instances using a One-R classifier trained on \n",
    "        training_df\n",
    "\n",
    "        Args:\n",
    "            training_df: pd.DataFrame of training instances\n",
    "            class_name: the name of the column of the class in training_df\n",
    "            testing_df: a pd.DataFrame of test instances\n",
    "\n",
    "        Returns:\n",
    "            A pd.Series of class predictions with the same index as testing_df\n",
    "    \"\"\"\n",
    "\n",
    "    #predict the most common class when an attribute is missing (Zero-R)\n",
    "    values, counts = np.unique(training_df[class_name], return_counts=True)\n",
    "    ind = np.argmax(counts)\n",
    "    most_common_class = values[ind]\n",
    "\n",
    "    #find the attribute which predicts class wih the lowest error rate\n",
    "    min_error_rate = float(\"inf\")\n",
    "    best_attr = None\n",
    "    best_predictor = None\n",
    "    for attr in training_df.columns.drop(class_name):\n",
    "\n",
    "        attr_groups = training_df[[attr, class_name]].groupby(attr).groups\n",
    "\n",
    "        attr_predictor = {np.nan: most_common_class}\n",
    "        for av, idx in attr_groups.items():\n",
    "            \n",
    "            #get most frequent class for av, choosing randomly if more than one\n",
    "            most_frequent_class = training_df[class_name][idx].mode()\n",
    "            choice = np.random.randint(len(most_frequent_class))\n",
    "\n",
    "            attr_predictor[av] = most_frequent_class.iloc[choice]\n",
    "\n",
    "        #check error rate of this attribute as predictor against training data\n",
    "        attr_predictions = np.empty(len(training_df), dtype=training_df[class_name].dtype)\n",
    "        for i, av in enumerate(training_df[attr]):\n",
    "            attr_predictions[i] = attr_predictor[av]\n",
    "\n",
    "        error_rate = np.count_nonzero(attr_predictions == training_df[class_name])\n",
    "\n",
    "        if error_rate < min_error_rate:\n",
    "            min_error_rate = error_rate\n",
    "            best_attr = attr\n",
    "            best_predictor = attr_predictor\n",
    "    \n",
    "    test_predictions = pd.Series(\n",
    "        np.empty(len(testing_df), dtype=training_df[class_name].dtype), \n",
    "        index=testing_df.index)\n",
    "\n",
    "    for i, row in testing_df.iterrows():\n",
    "        test_predictions[i] = best_predictor[row[best_attr]]\n",
    "    \n",
    "    return test_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Since it’s difficult to model the probabilities of ordinal data, ordinal attributes are often treated as either nominal variables or numeric variables. Compare these strategies on the ordinal datasets provided. Deterimine which approach gives higher classification accuracy and discuss why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy (you should implement this yourself and do not simply call existing implementations from `scikit-learn`). How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_split(total, k):\n",
    "    \"\"\" basically calculated the indices by which to slice the dataset into k partitions\"\"\"\n",
    "    quotient, remainder = divmod(total, k)\n",
    "    indices = [quotient + 1] * remainder + [quotient] * (k - remainder)\n",
    "    new = [indices[0]]\n",
    "    for i in range(1, k - 1):\n",
    "        new.append(new[i - 1] + indices[i])\n",
    "    return new\n",
    "\n",
    "def partition(df, k):\n",
    "    \"\"\"Splits the dataset into k partitions, and then allocates each in turn as the test set,\n",
    "        whilst the remainder are used for training\"\"\"\n",
    "    partition_lengths = k_split(df.shape[0], k)\n",
    "    partitions = np.array_split(df, partition_lengths)\n",
    "    splits = list()\n",
    "    for i in range(k):\n",
    "        train = pd.concat(partitions[:i] + partitions[i + 1:])\n",
    "        test = partitions[i]\n",
    "        splits.append((train, test))\n",
    "    return splits\n",
    "\n",
    "\n",
    "def cross_validation(df, cnfg, k):\n",
    "    results = np.zeros((4, 1))\n",
    "    for train, test in partition(df, k):\n",
    "        model = nb.train(train, cnfg[\"discrete\"], cnfg[\"numeric\"], cnfg[\"class_col\"])\n",
    "        predictions = nb.predict(test, model)\n",
    "        truth = test[cnfg[\"class_col\"]]\n",
    "        a, p, r, f = ev.evaluate(truth, predictions, print=False)\n",
    "        results[0] += a\n",
    "        results[1] += p\n",
    "        results[2] += r\n",
    "        results[3] += f\n",
    "    return results / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the na¨ıve Bayes classifier? Explain why, or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def laplace_smoothing(n_attr_obs, n_class_obs, n_attr_vals, alpha):\n",
    "    return (n_attr_obs + alpha) / (n_class_obs + alpha * n_attr_vals)\n",
    "\n",
    "def train_discrete_laplace(df, class_attr, class_vals, d_attr, alpha=1):\n",
    "    \"\"\" For training Naive Bayes on a discrete attribute, with laplace smoothing\n",
    "\n",
    "        For each class value computes the conditional probability of observing \n",
    "        each attribute value given that class value. \n",
    "\n",
    "        Args:\n",
    "            df: a pd.DataFrame\n",
    "            class_attr: the column in df of the class to predict\n",
    "            class_vals: the possible values of class_attr\n",
    "            d_attr: the column in df of the discrete attribute\n",
    "            alpha: the alpha value in laplace smoothing\n",
    "\n",
    "        Returns:\n",
    "            A dictionary, keyed by class values, of dictonaries storing the \n",
    "            conditional probability of observing each attribute value\n",
    "    \"\"\"\n",
    "\n",
    "    attr_vals = df[d_attr].unique()\n",
    "\n",
    "    params = dict()\n",
    "    for cv in class_vals:\n",
    "\n",
    "        params[cv] = dict()\n",
    "        cv_obs = df[df[class_attr] == cv]\n",
    "        for av in attr_vals:\n",
    "            num_av_obs = np.count_nonzero((cv_obs[d_attr] == av).to_numpy())\n",
    "            params[cv][av] = laplace_smoothing(num_av_obs, cv_obs.shape[0], len(attr_vals), alpha)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "The Gaussian na¨ıve Bayes classifier assumes that numeric attributes come from a Gaussian distribution. Is this assumption always true for the numeric attributes in these datasets? Identify some cases where the Gaussian assumption is violated and describe any evidence (or lack thereof) that this has some effect on the NB classifier’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}